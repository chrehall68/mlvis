{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting on Liar Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context'],\n",
       "        num_rows: 10269\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context'],\n",
       "        num_rows: 1283\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context'],\n",
       "        num_rows: 1284\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "liar = datasets.load_dataset(\"liar\")\n",
    "liar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = liar[\"train\"]\n",
    "test = liar[\"test\"]\n",
    "val = liar[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context'],\n",
       "    num_rows: 12836\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_liar = datasets.concatenate_datasets([train, test, val])\n",
    "full_liar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon = \"tiiuae/falcon-7b-instruct\"\n",
    "llama = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "mistral = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "orca = \"microsoft/Orca-2-7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this depending on experiment\n",
    "model_name = llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d8c46b5a904b66ae8bbd673588fbd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, quantization_config=config, device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "from typing import Dict\n",
    "\n",
    "LABEL_MAP = {\n",
    "    0: \"E\",  # 0 : False\n",
    "    1: \"C\",  # 1 : Half True\n",
    "    2: \"B\",  # 2 : Mostly True\n",
    "    3: \"A\",  # 3 : True\n",
    "    4: \"D\",  # 4 : Barely True\n",
    "    5: \"F\",  # 5 : Pants on Fire\n",
    "}\n",
    "\n",
    "\n",
    "def was_correct(\n",
    "    decoded:str, entry: Dict[str, int]\n",
    ") -> bool:\n",
    "    return LABEL_MAP[entry[\"label\"]] in decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1770)\n",
    "entries = random.choices(list(range(len(train))), k=n_examples)\n",
    "\n",
    "def to_zero_shot_prompt(entry: Dict[str, str]) -> str:\n",
    "    speaker = entry[\"speaker\"].replace(\"-\", \" \").title()\n",
    "    statement = entry[\"statement\"].lstrip(\"Says \")\n",
    "\n",
    "    prompt = f\"\"\"Please select the option that most closely describes the following claim by {speaker}:\\n{statement}\\n\\nA) True\\nB) Mostly True\\nC) Half True\\nD) Barely True\\nE) False\\nF) Pants on Fire (absurd lie)\\n\\nChoice: (\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def to_n_shot_prompt(n: int, entry: Dict[str, str]) -> str:\n",
    "    examples = \"\"\n",
    "    for i in range(n):\n",
    "        examples += to_zero_shot_prompt(train[entries[i]]) + LABEL_MAP[train[entries[i]]['label']] + \"\\n\\n\"\n",
    "    prompt = to_zero_shot_prompt(entry)\n",
    "    return examples + prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def workflow(entry: dict, model, k:int=0, verbose: bool = False) -> bool:\n",
    "    prompt = to_n_shot_prompt(k, entry)\n",
    "\n",
    "    # encode input, move it to cuda, then generate\n",
    "    encoded_input = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    encoded_input = {item: val.cuda() for item, val in encoded_input.items()}\n",
    "    generation = model.generate(\n",
    "        **encoded_input,\n",
    "        max_new_tokens=1,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # log the prompt and response if verbose\n",
    "    if verbose:\n",
    "        print(tokenizer.batch_decode(generation)[0])\n",
    "\n",
    "    decoded = tokenizer.decode(generation[0, -1])\n",
    "    correct = was_correct(decoded, entry)\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"The model was\",\n",
    "            \"correct\" if correct else \"incorrect\",\n",
    "            \" - responded\",\n",
    "            tokenizer.decode(generation[0, -1]),\n",
    "            \"and answer should have been\",\n",
    "            LABEL_MAP[entry[\"label\"]],\n",
    "        )\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eliot/miniconda3/envs/torch/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/eliot/miniconda3/envs/torch/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Please select the option that most closely describes the following claim by Bernie S:\n",
      "Today the Walton family of Walmart own more wealth than the bottom 40 percent of America.\n",
      "\n",
      "A) True\n",
      "B) Mostly True\n",
      "C) Half True\n",
      "D) Barely True\n",
      "E) False\n",
      "F) Pants on Fire (absurd lie)\n",
      "\n",
      "Choice: (A\n",
      "\n",
      "Please select the option that most closely describes the following claim by Kim Guadagno:\n",
      "Panasonic stayed in New Jersey because of the Business Retention and Relocation Assistance Grant (BRRAG) program.\n",
      "\n",
      "A) True\n",
      "B) Mostly True\n",
      "C) Half True\n",
      "D) Barely True\n",
      "E) False\n",
      "F) Pants on Fire (absurd lie)\n",
      "\n",
      "Choice: (E\n",
      "\n",
      "Please select the option that most closely describes the following claim by Democratic Governors Association:\n",
      "Lincoln Chafee voted with President George W. Bush and the conservative leadership 76% of the time.\n",
      "\n",
      "A) True\n",
      "B) Mostly True\n",
      "C) Half True\n",
      "D) Barely True\n",
      "E) False\n",
      "F) Pants on Fire (absurd lie)\n",
      "\n",
      "Choice: (D\n",
      "\n",
      "Please select the option that most closely describes the following claim by Patrick Kennedy:\n",
      "\"One-third of the health care dollar goes to no such thing as health care; it goes to the insurance companies.\"\n",
      "\n",
      "A) True\n",
      "B) Mostly True\n",
      "C) Half True\n",
      "D) Barely True\n",
      "E) False\n",
      "F) Pants on Fire (absurd lie)\n",
      "\n",
      "Choice: (E\n",
      "\n",
      "Please select the option that most closely describes the following claim by Hillary Clinton:\n",
      "Risk analysts listed Donald Trump, a Donald Trump presidency, as one of the top threats facing the global economy, ahead of terrorism.\n",
      "\n",
      "A) True\n",
      "B) Mostly True\n",
      "C) Half True\n",
      "D) Barely True\n",
      "E) False\n",
      "F) Pants on Fire (absurd lie)\n",
      "\n",
      "Choice: (A\n",
      "\n",
      "Please select the option that most closely describes the following claim by Roy Barnes:\n",
      "When Roy Barnes was governor, \"Georgia created 235,000 jobs.\"\n",
      "\n",
      "A) True\n",
      "B) Mostly True\n",
      "C) Half True\n",
      "D) Barely True\n",
      "E) False\n",
      "F) Pants on Fire (absurd lie)\n",
      "\n",
      "Choice: (B\n",
      "The model was incorrect  - responded B and answer should have been C\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "workflow(train[random.randint(0, len(train) - 1)], model, verbose=True, k=n_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of zero-shot prompting the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_correct = 0\n",
    "answers = {}\n",
    "for idx, entry in enumerate(tqdm(full_liar)):\n",
    "    if idx in entries:\n",
    "        continue  # don't include items that were in the examples\n",
    "    \n",
    "    correct = workflow(entry, model, k=n_examples)\n",
    "    if correct:\n",
    "        num_correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log results\n",
    "with open(f\"{n_examples}_shot.txt\", \"a\") as file:\n",
    "    file.write(f\"{model_name} : {num_correct}/{len(full_liar)-len(entries)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results up till now\n",
    "with open(f\"{n_examples}_shot.txt\", \"r\") as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
