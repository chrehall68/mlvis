{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boolean Prompting on Liar Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context'],\n",
       "        num_rows: 10269\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context'],\n",
       "        num_rows: 1283\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context'],\n",
       "        num_rows: 1284\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "liar = datasets.load_dataset(\"liar\")\n",
    "liar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = liar[\"train\"]\n",
    "test = liar[\"test\"]\n",
    "val = liar[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/016854656/miniconda3/envs/torch/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context'],\n",
       "    num_rows: 12836\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_liar = datasets.concatenate_datasets([train, test, val])\n",
    "full_liar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon = \"tiiuae/falcon-7b-instruct\"\n",
    "llama = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "mistral = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "orca = \"microsoft/Orca-2-7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this depending on experiment\n",
    "model_name = mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, quantization_config=config, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "from typing import Dict\n",
    "\n",
    "# split into True/False\n",
    "LABEL_MAP = {\n",
    "    0:\"A\",\n",
    "    1:\"B\"\n",
    "}\n",
    "\n",
    "# 0 : False\n",
    "# 1 : Half True\n",
    "# 2 : Mostly True\n",
    "# 3 : True\n",
    "# 4 : Barely True\n",
    "# 5 : Pants on Fire\n",
    "def to_binary_label(entry):\n",
    "    if entry['label'] in [3, 2, 1]:\n",
    "        entry['label'] = 0\n",
    "    else:\n",
    "        entry['label'] = 1\n",
    "    return entry\n",
    "\n",
    "def was_correct(\n",
    "    decoded:str, entry: Dict[str, int]\n",
    ") -> bool:\n",
    "    return LABEL_MAP[entry[\"label\"]] in decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_liar = full_liar.map(to_binary_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1770)\n",
    "entries = random.choices(list(range(len(full_liar))), k=n_examples)\n",
    "\n",
    "def to_zero_shot_prompt(entry: Dict[str, str]) -> str:\n",
    "    speaker = entry[\"speaker\"].replace(\"-\", \" \").title()\n",
    "    statement = entry[\"statement\"].lstrip(\"Says \")\n",
    "\n",
    "    prompt = f\"\"\"Please select the option that most closely describes the following claim by {speaker}:\\n{statement}\\n\\nA) True\\nB) False\\n\\nChoice: (\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def to_n_shot_prompt(n: int, entry: Dict[str, str]) -> str:\n",
    "    examples = \"\"\n",
    "    for i in range(n):\n",
    "        examples += to_zero_shot_prompt(full_liar[entries[i]]) + LABEL_MAP[full_liar[entries[i]]['label']] + \"\\n\\n\"\n",
    "    prompt = to_zero_shot_prompt(entry)\n",
    "    return examples + prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses: Dict[str, list] = {}\n",
    "def workflow(idx, entry: dict, model, k:int=0, verbose: bool = False) -> bool:\n",
    "    prompt = to_n_shot_prompt(k, entry)\n",
    "\n",
    "    # encode input, move it to cuda, then generate\n",
    "    encoded_input = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    encoded_input = {item: val.cuda() for item, val in encoded_input.items()}\n",
    "    generation = model.generate(\n",
    "        **encoded_input,\n",
    "        max_new_tokens=1,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # log the prompt and response if verbose\n",
    "    if verbose:\n",
    "        print(tokenizer.batch_decode(generation)[0])\n",
    "\n",
    "    decoded = tokenizer.decode(generation[0, -1])\n",
    "    correct = was_correct(decoded, entry)\n",
    "\n",
    "    if decoded not in responses:\n",
    "        responses[decoded] = []\n",
    "    responses[decoded].append(idx)\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"The model was\",\n",
    "            \"correct\" if correct else \"incorrect\",\n",
    "            \" - responded\",\n",
    "            tokenizer.decode(generation[0, -1]),\n",
    "            \"and answer should have been\",\n",
    "            LABEL_MAP[entry[\"label\"]],\n",
    "        )\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Please select the option that most closely describes the following claim by Jorge Elorza:\n",
      "The reality is that we have roughly 15,000 undocumented immigrants living in the state...\n",
      "\n",
      "A) True\n",
      "B) False\n",
      "\n",
      "Choice: (A\n",
      "The model was correct  - responded A and answer should have been A\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "workflow(0, train[random.randint(0, len(train) - 1)], model, verbose=True, k=n_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of zero-shot prompting the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████| 12836/12836 [31:06<00:00,  6.88it/s, acc: 0.569]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_correct = 0\n",
    "responses = {}\n",
    "for idx, entry in enumerate((prog := tqdm(full_liar))):\n",
    "    if idx in entries:\n",
    "        continue  # don't include items that were in the examples\n",
    "    \n",
    "    correct = workflow(idx, entry, model, k=n_examples)\n",
    "    if correct:\n",
    "        num_correct += 1\n",
    "    prog.set_postfix_str(f\"acc: {num_correct/(idx+1):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log results\n",
    "with open(f\"{n_examples}_shot_binary.txt\", \"a\") as file:\n",
    "    file.write(f\"{model_name} : {num_correct}/{len(full_liar)-len(entries)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results up till now\n",
    "with open(f\"{n_examples}_shot_binary.txt\", \"r\") as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(responses, open(f\"{model_name[model_name.index('/')+1:]}_responses_bool.pk\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
