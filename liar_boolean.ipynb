{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting on Liar Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context'],\n",
       "        num_rows: 10269\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context'],\n",
       "        num_rows: 1283\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context'],\n",
       "        num_rows: 1284\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "liar = datasets.load_dataset(\"liar\")\n",
    "liar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = liar[\"train\"]\n",
    "test = liar[\"test\"]\n",
    "val = liar[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/016854656/miniconda3/envs/torch/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context'],\n",
       "    num_rows: 12836\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_liar = datasets.concatenate_datasets([train, test, val])\n",
    "full_liar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon = \"tiiuae/falcon-7b-instruct\"\n",
    "llama = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "mistral = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "orca = \"microsoft/Orca-2-7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this depending on experiment\n",
    "model_name = llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/016854656/miniconda3/envs/torch/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/016854656/miniconda3/envs/torch/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/016854656/miniconda3/envs/torch/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d22f250cc444e058ba14b72f03a5374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, quantization_config=config, device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "from typing import Dict\n",
    "\n",
    "# split into True/False\n",
    "LABEL_MAP = {\n",
    "    0:\"A\",\n",
    "    1:\"B\"\n",
    "}\n",
    "\n",
    "# 0 : False\n",
    "# 1 : Half True\n",
    "# 2 : Mostly True\n",
    "# 3 : True\n",
    "# 4 : Barely True\n",
    "# 5 : Pants on Fire\n",
    "def to_binary_label(entry):\n",
    "    if entry['label'] in [3, 2, 1]:\n",
    "        entry['label'] = 0\n",
    "    else:\n",
    "        entry['label'] = 1\n",
    "    return entry\n",
    "\n",
    "def was_correct(\n",
    "    decoded:str, entry: Dict[str, int]\n",
    ") -> bool:\n",
    "    return LABEL_MAP[entry[\"label\"]] in decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_liar = full_liar.map(to_binary_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1770)\n",
    "entries = random.choices(list(range(len(full_liar))), k=n_examples)\n",
    "\n",
    "def to_zero_shot_prompt(entry: Dict[str, str]) -> str:\n",
    "    speaker = entry[\"speaker\"].replace(\"-\", \" \").title()\n",
    "    statement = entry[\"statement\"].lstrip(\"Says \")\n",
    "\n",
    "    prompt = f\"\"\"Please select the option that most closely describes the following claim by {speaker}:\\n{statement}\\n\\nA) True\\nB) False\\n\\nChoice: (\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def to_n_shot_prompt(n: int, entry: Dict[str, str]) -> str:\n",
    "    examples = \"\"\n",
    "    for i in range(n):\n",
    "        examples += to_zero_shot_prompt(full_liar[entries[i]]) + LABEL_MAP[full_liar[entries[i]]['label']] + \"\\n\\n\"\n",
    "    prompt = to_zero_shot_prompt(entry)\n",
    "    return examples + prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses: Dict[str, list] = {}\n",
    "def workflow(idx, entry: dict, model, k:int=0, verbose: bool = False) -> bool:\n",
    "    prompt = to_n_shot_prompt(k, entry)\n",
    "\n",
    "    # encode input, move it to cuda, then generate\n",
    "    encoded_input = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    encoded_input = {item: val.cuda() for item, val in encoded_input.items()}\n",
    "    generation = model.generate(\n",
    "        **encoded_input,\n",
    "        max_new_tokens=1,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # log the prompt and response if verbose\n",
    "    if verbose:\n",
    "        print(tokenizer.batch_decode(generation)[0])\n",
    "\n",
    "    decoded = tokenizer.decode(generation[0, -1])\n",
    "    correct = was_correct(decoded, entry)\n",
    "\n",
    "    if decoded not in responses:\n",
    "        responses[decoded] = []\n",
    "    responses[decoded].append(idx)\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"The model was\",\n",
    "            \"correct\" if correct else \"incorrect\",\n",
    "            \" - responded\",\n",
    "            tokenizer.decode(generation[0, -1]),\n",
    "            \"and answer should have been\",\n",
    "            LABEL_MAP[entry[\"label\"]],\n",
    "        )\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/016854656/miniconda3/envs/torch/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/016854656/miniconda3/envs/torch/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Please select the option that most closely describes the following claim by Kendrick Meek:\n",
      "Warren Buffett called credit default swaps financial weapons of mass destruction. And Greene was the first individual to use them.\n",
      "\n",
      "A) True\n",
      "B) False\n",
      "\n",
      "Choice: (A\n",
      "\n",
      "Please select the option that most closely describes the following claim by Bill Nelson:\n",
      "Many state and federal agencies have such navigators involved in helping folks maneuver through the often complex processes associated with filing benefits claims, for example -- even buying health insurance.\n",
      "\n",
      "A) True\n",
      "B) False\n",
      "\n",
      "Choice: (A\n",
      "\n",
      "Please select the option that most closely describes the following claim by George Lemieux:\n",
      "(George) LeMieux never requested a single earmark and pushed to ban them all.\n",
      "\n",
      "A) True\n",
      "B) False\n",
      "\n",
      "Choice: (A\n",
      "\n",
      "Please select the option that most closely describes the following claim by Bill Oreilly:\n",
      "We researched to find out if anybody on Fox News had ever said youre going to jail if you dont buy health insurance. Nobodys ever said it.\n",
      "\n",
      "A) True\n",
      "B) False\n",
      "\n",
      "Choice: (B\n",
      "\n",
      "Please select the option that most closely describes the following claim by Callista Gingrich:\n",
      "President Barack Obama has weakened the respect for America abroad.\n",
      "\n",
      "A) True\n",
      "B) False\n",
      "\n",
      "Choice: (B\n",
      "\n",
      "Please select the option that most closely describes the following claim by John Mccain:\n",
      "\"Barack Obama said he would debate 'anywhere, anytime' but has rejected joint town hall meetings.\"\n",
      "\n",
      "A) True\n",
      "B) False\n",
      "\n",
      "Choice: (B\n",
      "The model was correct  - responded B and answer should have been B\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "workflow(0, train[random.randint(0, len(train) - 1)], model, verbose=True, k=n_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of zero-shot prompting the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/12836 [00:00<?, ?it/s]/home/016854656/miniconda3/envs/torch/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/016854656/miniconda3/envs/torch/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████| 12836/12836 [53:29<00:00,  4.00it/s, acc: 0.505]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_correct = 0\n",
    "responses = {}\n",
    "for idx, entry in enumerate((prog := tqdm(full_liar))):\n",
    "    if idx in entries:\n",
    "        continue  # don't include items that were in the examples\n",
    "    \n",
    "    correct = workflow(idx, entry, model, k=n_examples)\n",
    "    if correct:\n",
    "        num_correct += 1\n",
    "    prog.set_postfix_str(f\"acc: {num_correct/(idx+1):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log results\n",
    "with open(f\"{n_examples}_shot.txt\", \"a\") as file:\n",
    "    file.write(f\"{model_name} : {num_correct}/{len(full_liar)-len(entries)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results up till now\n",
    "with open(f\"{n_examples}_shot.txt\", \"r\") as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(responses, open(f\"{model_name[model_name.index('/')+1:]}_responses_bool.pk\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
