{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Captum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/016854656/miniconda3/envs/torch/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context'],\n",
       "    num_rows: 12836\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "liar = datasets.load_dataset(\"liar\")\n",
    "full_liar = datasets.concatenate_datasets(\n",
    "    [liar[\"train\"], liar[\"test\"], liar[\"validation\"]]\n",
    ")\n",
    "full_liar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import captum\n",
    "import captum.attr as attr\n",
    "from captum.attr import visualization as viz\n",
    "from captum._utils.models.linear_model import SkLearnLasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon = \"tiiuae/falcon-7b-instruct\"\n",
    "llama = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "mistral = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "orca = \"microsoft/Orca-2-7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/016854656/miniconda3/envs/torch/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/016854656/miniconda3/envs/torch/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/016854656/miniconda3/envs/torch/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90ad8279d5f14069b798edf7a4ffe0cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, quantization_config=config, # device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "LABEL_MAP = {\n",
    "    0: \"E\",  # 0 : False\n",
    "    1: \"C\",  # 1 : Half True\n",
    "    2: \"B\",  # 2 : Mostly True\n",
    "    3: \"A\",  # 3 : True\n",
    "    4: \"D\",  # 4 : Barely True\n",
    "    5: \"F\",  # 5 : Pants on Fire\n",
    "}\n",
    "\n",
    "\n",
    "def was_correct(decoded: str, entry: Dict[str, int]) -> bool:\n",
    "    return LABEL_MAP[entry[\"label\"]] in decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Dict\n",
    "\n",
    "random.seed(1770)\n",
    "entries = random.choices(list(range(len(full_liar))), k=n_examples)\n",
    "\n",
    "\n",
    "def to_zero_shot_prompt(entry: Dict[str, str]) -> str:\n",
    "    speaker = entry[\"speaker\"].replace(\"-\", \" \").title()\n",
    "    statement = entry[\"statement\"].lstrip(\"Says \")\n",
    "\n",
    "    prompt = f\"\"\"Please select the option that most closely describes the following claim by {speaker}:\\n{statement}\\n\\nA) True\\nB) Mostly True\\nC) Half True\\nD) Barely True\\nE) False\\nF) Pants on Fire (absurd lie)\\n\\nChoice: (\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def to_n_shot_prompt(n: int, entry: Dict[str, str]) -> str:\n",
    "    examples = \"\"\n",
    "    for i in range(n):\n",
    "        examples += (\n",
    "            to_zero_shot_prompt(full_liar[entries[i]])\n",
    "            + LABEL_MAP[full_liar[entries[i]][\"label\"]]\n",
    "            + \"\\n\\n\"\n",
    "        )\n",
    "    prompt = to_zero_shot_prompt(entry)\n",
    "    return examples + prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Please select the option that most closely describes the following claim by Jorge Elorza:\n",
      "The reality is that we have roughly 15,000 undocumented immigrants living in the state...\n",
      "\n",
      "A) True\n",
      "B) Mostly True\n",
      "C) Half True\n",
      "D) Barely True\n",
      "E) False\n",
      "F) Pants on Fire (absurd lie)\n",
      "\n",
      "Choice: (A) True\n",
      "\n",
      "Explanation: Jorge Elorza's statement is based on a report from the Rhode Island Department of Health, which estimated that there were approximately 15,000 undocumented immigrants living in the state in 2013. This estimate was derived from data on the number of uninsured residents, assuming a certain percentage were undocumented. While there may be some margin of error in this estimate, it is generally accepted as a reliable approximation of the number of undocumented immigrants in Rhode Island. Therefore, Jorge Elorza's statement is true.</s>\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "k = random.randint(0, len(full_liar))\n",
    "generation = tokenizer.batch_decode(\n",
    "        model.generate(\n",
    "            tokenizer(\n",
    "                to_zero_shot_prompt(full_liar[k]), return_tensors=\"pt\"\n",
    "            ).input_ids.cuda(),\n",
    "            max_new_tokens=200,\n",
    "        )\n",
    "    )[0]\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Please select the option that most closely describes the following claim by Jorge Elorza:\n",
      "The reality is that we have roughly 15,000 undocumented immigrants living in the state...\n",
      "\n",
      "A) True\n",
      "B) Mostly True\n",
      "C) Half True\n",
      "D) Barely True\n",
      "E) False\n",
      "F) Pants on Fire (absurd lie)\n",
      "\n",
      "Choice: (A) True\n",
      "\n",
      "Explanation: Jorge Elorza's statement is based on a report from the Rhode Island Department of Health, which estimated that there were approximately 15,000 undocumented immigrants living in the state in 2013. This estimate was derived from data on the number of uninsured residents, assuming a certain percentage were undocumented. While there may be some margin of error in this estimate, it is generally accepted as a reliable approximation of the number of undocumented immigrants in Rhode Island. Therefore, Jorge Elorza's statement is true.</s> \n",
      "\n",
      "Explain your answer:\n",
      "\n",
      "Jorge Elorza's statement that \"we have roughly 15,000 undocumented immigrants living in the state\" is based on a report from the Rhode Island Department of Health. The report estimated the number of undocumented immigrants in the state based on data on the number of uninsured residents, assuming a certain percentage were undocumented. This estimate is generally accepted as a reliable approximation of the number of undocumented immigrants in Rhode Island. Therefore, Jorge Elorza's statement is true.</s>\n"
     ]
    }
   ],
   "source": [
    "continuation = f\"\"\"{generation.lstrip('<s> ')}\n",
    "\n",
    "Explain your answer:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\n",
    "    tokenizer.batch_decode(\n",
    "        model.generate(\n",
    "            tokenizer(\n",
    "                continuation, return_tensors=\"pt\"\n",
    "            ).input_ids.cuda(),\n",
    "            max_new_tokens=200,\n",
    "        )\n",
    "    )[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': [28741],\n",
       " 'B': [28760],\n",
       " 'C': [28743],\n",
       " 'D': [28757],\n",
       " 'E': [28749],\n",
       " 'F': [28765]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = tokenizer.vocab\n",
    "label_tokens = {\n",
    "    \"A\": [],\n",
    "    \"B\": [],\n",
    "    \"C\": [],\n",
    "    \"D\": [],\n",
    "    \"E\": [],\n",
    "    \"F\": [],\n",
    "}\n",
    "for char, idx in vocab.items():\n",
    "    if char in label_tokens:\n",
    "        label_tokens[char].append(idx)\n",
    "label_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 28741, 'B': 28760, 'C': 28743, 'D': 28757, 'E': 28749, 'F': 28765}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_tokens = {char: idxs[0] for char, idxs in label_tokens.items()}\n",
    "label_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = to_n_shot_prompt(n_examples, full_liar[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 85])\n"
     ]
    }
   ],
   "source": [
    "# get tokens for later\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "print(tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,  5919,  5339,   272,  3551,   369,  1080, 11640, 13966,   272,\n",
       "          2296,  3452,   486, 26955,  1744,   271,  2166, 28747,    13,  1014,\n",
       "          6940,   349,   369,   478,   506, 15756, 28705, 28740, 28782, 28725,\n",
       "         28734, 28734, 28734,   640,  2048,   286, 22475,  3687,   297,   272,\n",
       "          1665,  1101,    13,    13, 28741, 28731,  6110,    13, 28760, 28731,\n",
       "          4822,   346,  6110,    13, 28743, 28731, 18994,  6110,    13, 28757,\n",
       "         28731,   365,  6672,  6110,    13, 28749, 28731,  8250,    13, 28765,\n",
       "         28731,   367,  1549,   356,  8643,   325,  4737, 12725,  4852, 28731,\n",
       "            13,    13, 28456, 28747,   325]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIME / SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIME = \"LIME\"\n",
    "SHAP = \"SHAP\"\n",
    "EXPERIMENT_TYPE = LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_results(tokens: torch.Tensor):\n",
    "    with torch.no_grad():\n",
    "        if tokenizer.bos_token_id is not None:  # falcon's is None\n",
    "            tokens[0, 0] = tokenizer.bos_token_id\n",
    "        result = model(\n",
    "            torch.where(tokens != 0, tokens, tokenizer.eos_token_id).cuda(),\n",
    "            attention_mask=torch.where(tokens != 0, 1, 0).cuda()\n",
    "        ).logits\n",
    "        ret = torch.nn.functional.softmax(result[:, -1], dim=-1).cpu()\n",
    "        assert not ret.isnan().any()\n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_embeds(tokens: torch.Tensor):\n",
    "    with torch.no_grad():\n",
    "        if hasattr(model, \"model\"):\n",
    "            return model.model.embed_tokens(tokens.cuda())\n",
    "        elif hasattr(model, \"transformer\"):\n",
    "            return model.transformer.word_embeddings(tokens.cuda())\n",
    "        raise Exception(\"Unknown model format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode text indices into latent representations & calculate cosine similarity\n",
    "def exp_embedding_cosine_distance(original_inp, perturbed_inp, _, **kwargs):\n",
    "    original_emb = get_embeds(original_inp)\n",
    "    perturbed_emb = get_embeds(perturbed_inp)\n",
    "    distance = 1 - F.cosine_similarity(original_emb, perturbed_emb, dim=-1)\n",
    "    distance[distance.isnan()] = 0\n",
    "    ret = torch.exp(-1 * (distance**2) / 2).sum()\n",
    "    assert not ret.isnan().any()\n",
    "    return ret\n",
    "\n",
    "\n",
    "# binary vector where each word is selected independently and uniformly at random\n",
    "i = 0\n",
    "\n",
    "\n",
    "def bernoulli_perturb(text, **kwargs):\n",
    "    global i\n",
    "    probs = torch.ones_like(text) * 0.5\n",
    "    probs[0, 0] = 0  # don't get rid of the start token\n",
    "    ret = torch.bernoulli(probs).long()\n",
    "    i += 1\n",
    "    return ret\n",
    "\n",
    "\n",
    "# remove absent tokens based on the intepretable representation sample\n",
    "def interp_to_input(interp_sample, original_input, **kwargs):\n",
    "    ret = original_input.clone()\n",
    "    ret[interp_sample.bool()] = 0\n",
    "    return ret\n",
    "\n",
    "\n",
    "if EXPERIMENT_TYPE == LIME:\n",
    "    attributers = {\n",
    "        char: attr.LimeBase(\n",
    "            softmax_results,\n",
    "            interpretable_model=SkLearnLasso(alpha=0.0005),\n",
    "            similarity_func=exp_embedding_cosine_distance,\n",
    "            perturb_func=bernoulli_perturb,\n",
    "            perturb_interpretable_space=True,\n",
    "            from_interp_rep_transform=interp_to_input,\n",
    "            to_interp_rep_transform=None,\n",
    "        )\n",
    "        for char in label_tokens.keys()\n",
    "    }\n",
    "elif EXPERIMENT_TYPE == SHAP:\n",
    "    attributers = {\n",
    "        char : attr.KernelShap(softmax_results)\n",
    "        for char in label_tokens.keys()\n",
    "    }\n",
    "else:\n",
    "    raise Exception(\"Invalid Experiment Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efc7648781424d98a16aed9ac91cad53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lime Base attribution:   0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9d4473f97d84887861a0e69c4eb9fef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lime Base attribution:   0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m attributions \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m char, label_token \u001b[38;5;129;01min\u001b[39;00m label_tokens\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m----> 3\u001b[0m     attributions[char] \u001b[38;5;241m=\u001b[39m \u001b[43mattributers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchar\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattribute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/captum/log/__init__.py:42\u001b[0m, in \u001b[0;36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/captum/attr/_core/lime.py:484\u001b[0m, in \u001b[0;36mLimeBase.attribute\u001b[0;34m(self, inputs, target, additional_forward_args, n_samples, perturbations_per_eval, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expanded_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    482\u001b[0m     expanded_target \u001b[38;5;241m=\u001b[39m _expand_target(target, \u001b[38;5;28mlen\u001b[39m(curr_model_inputs))\n\u001b[0;32m--> 484\u001b[0m model_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcurr_model_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpanded_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpanded_additional_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m show_progress:\n\u001b[1;32m    492\u001b[0m     attr_progress\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/captum/attr/_core/lime.py:540\u001b[0m, in \u001b[0;36mLimeBase._evaluate_batch\u001b[0;34m(self, curr_model_inputs, expanded_target, expanded_additional_args, device)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_evaluate_batch\u001b[39m(\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    535\u001b[0m     curr_model_inputs: List[TensorOrTupleOfTensorsGeneric],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    538\u001b[0m     device: torch\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[1;32m    539\u001b[0m ):\n\u001b[0;32m--> 540\u001b[0m     model_out \u001b[38;5;241m=\u001b[39m \u001b[43m_run_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_reduce_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr_model_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpanded_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpanded_additional_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model_out, Tensor):\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m model_out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(curr_model_inputs), (\n\u001b[1;32m    548\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of outputs is not appropriate, must return \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    549\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mone output per perturbed input\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    550\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/captum/_utils/common.py:531\u001b[0m, in \u001b[0;36m_run_forward\u001b[0;34m(forward_func, inputs, target, additional_forward_args)\u001b[0m\n\u001b[1;32m    528\u001b[0m inputs \u001b[38;5;241m=\u001b[39m _format_inputs(inputs)\n\u001b[1;32m    529\u001b[0m additional_forward_args \u001b[38;5;241m=\u001b[39m _format_additional_forward_args(additional_forward_args)\n\u001b[0;32m--> 531\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mforward_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _select_targets(output, target)\n",
      "Cell \u001b[0;32mIn[18], line 9\u001b[0m, in \u001b[0;36msoftmax_results\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m      4\u001b[0m         tokens[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbos_token_id\n\u001b[1;32m      5\u001b[0m     result \u001b[38;5;241m=\u001b[39m model(\n\u001b[1;32m      6\u001b[0m         torch\u001b[38;5;241m.\u001b[39mwhere(tokens \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m, tokens, tokenizer\u001b[38;5;241m.\u001b[39meos_token_id)\u001b[38;5;241m.\u001b[39mcuda(),\n\u001b[1;32m      7\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mwhere(tokens \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      8\u001b[0m     )\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m----> 9\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret\u001b[38;5;241m.\u001b[39misnan()\u001b[38;5;241m.\u001b[39many()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "attributions = {}\n",
    "for char, label_token in label_tokens.items():\n",
    "    attributions[char] = attributers[char].attribute(\n",
    "        tokens, target=label_token, n_samples=512, show_progress=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 85])\n"
     ]
    }
   ],
   "source": [
    "print(attributions[\"A\"].shape)\n",
    "assert attributions[\"A\"].nonzero().numel() != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([85])\n",
      "['<s>', 'Please', 'select', 'the', 'option', 'that', 'most', 'closely', 'describes', 'the', 'following', 'claim', 'by', 'Jorge', 'El', 'or', 'za', ':', '\\n', 'The', 'reality', 'is', 'that', 'we', 'have', 'roughly', '', '1', '5', ',', '0', '0', '0', 'und', 'ocument', 'ed', 'immigrants', 'living', 'in', 'the', 'state', '...', '\\n', '\\n', 'A', ')', 'True', '\\n', 'B', ')', 'Most', 'ly', 'True', '\\n', 'C', ')', 'Half', 'True', '\\n', 'D', ')', 'B', 'arely', 'True', '\\n', 'E', ')', 'False', '\\n', 'F', ')', 'P', 'ants', 'on', 'Fire', '(', 'abs', 'urd', 'lie', ')', '\\n', '\\n', 'Choice', ':', '(']\n"
     ]
    }
   ],
   "source": [
    "all_tokens = tokens.squeeze(0)\n",
    "print(all_tokens.shape)\n",
    "all_tokens = list(map(tokenizer.decode, all_tokens))\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32000])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    predictions = softmax_results(tokens)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CustomDataRecord:\n",
    "    word_attributions: Any\n",
    "    pred_prob: torch.Tensor\n",
    "    pred_class: str\n",
    "    attr_class: str\n",
    "    attr_prob: torch.Tensor\n",
    "    attr_score: Any\n",
    "    raw_input_ids: Any | list[str]\n",
    "    convergence_delta: Any = None\n",
    "\n",
    "SCALE = 35\n",
    "\n",
    "attr_vis = [\n",
    "    CustomDataRecord(\n",
    "        attributions[char][0] * SCALE,  # word attributions\n",
    "        predictions[0].max(),  # predicted probability\n",
    "        tokenizer.decode(torch.argmax(predictions[0])),  # predicted class\n",
    "        char,  # attr class\n",
    "        predictions[0, label_tokens[char]],  # attr probability\n",
    "        attributions[char].sum(),  # attr score\n",
    "        all_tokens,  # raw input ids\n",
    "        abs(predictions[0, label_tokens[char]] - attributions[char].sum()) if EXPERIMENT_TYPE == SHAP else None\n",
    "    )\n",
    "    for char in attributions.keys()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "def visualize_text(\n",
    "    datarecords: list[CustomDataRecord], legend: bool = True\n",
    ") -> \"HTML\":  # In quotes because this type doesn't exist in standalone mode\n",
    "    dom = [\"<table width: 100%>\"]\n",
    "    if EXPERIMENT_TYPE == LIME:\n",
    "        rows = [\n",
    "            \"<tr><th>Predicted Label</th>\"\n",
    "            \"<th>Attribution Label</th>\"\n",
    "            \"<th>Word Importance</th>\"\n",
    "        ]\n",
    "    else:\n",
    "        rows = [\n",
    "            \"<tr><th>Predicted Label</th>\"\n",
    "            \"<th>Attribution Label</th>\"\n",
    "            \"<th>Convergence Delta</th>\"\n",
    "            \"<th>Word Importance</th>\"\n",
    "        ]\n",
    "    for datarecord in datarecords:\n",
    "        if EXPERIMENT_TYPE == LIME:\n",
    "            rows.append(\n",
    "                \"\".join(\n",
    "                    [\n",
    "                        \"<tr>\",\n",
    "                        viz.format_classname(\n",
    "                            \"{0} ({1:.2f})\".format(\n",
    "                                datarecord.pred_class, datarecord.pred_prob\n",
    "                            )\n",
    "                        ),\n",
    "                        viz.format_classname(\n",
    "                            \"{0} ({1:.2f})\".format(\n",
    "                                datarecord.attr_class, datarecord.attr_prob\n",
    "                            )\n",
    "                        ),\n",
    "                        viz.format_word_importances(\n",
    "                            datarecord.raw_input_ids, datarecord.word_attributions\n",
    "                        ),\n",
    "                        \"<tr>\",\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            rows.append(\n",
    "                \"\".join(\n",
    "                    [\n",
    "                        \"<tr>\",\n",
    "                        viz.format_classname(\n",
    "                            \"{0} ({1:.2f})\".format(\n",
    "                                datarecord.pred_class, datarecord.pred_prob\n",
    "                            )\n",
    "                        ),\n",
    "                        viz.format_classname(\n",
    "                            \"{0} ({1:.2f})\".format(\n",
    "                                datarecord.attr_class, datarecord.attr_prob\n",
    "                            )\n",
    "                        ),\n",
    "                        viz.format_classname(\"{0:.2f}\".format(datarecord.convergence_delta)),\n",
    "                        viz.format_word_importances(\n",
    "                            datarecord.raw_input_ids, datarecord.word_attributions\n",
    "                        ),\n",
    "                        \"<tr>\",\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "    if legend:\n",
    "        dom.append(\n",
    "            '<div style=\"border-top: 1px solid; margin-top: 5px; \\\n",
    "            padding-top: 5px; display: inline-block\">'\n",
    "        )\n",
    "        dom.append(\"<b>Legend: </b>\")\n",
    "\n",
    "        for value, label in zip([-1, 0, 1], [\"Negative\", \"Neutral\", \"Positive\"]):\n",
    "            dom.append(\n",
    "                '<span style=\"display: inline-block; width: 10px; height: 10px; \\\n",
    "                border: 1px solid; background-color: \\\n",
    "                {value}\"></span> {label}  '.format(\n",
    "                    value=viz._get_color(value), label=label\n",
    "                )\n",
    "            )\n",
    "        dom.append(\"</div>\")\n",
    "\n",
    "    dom.append(\"\".join(rows))\n",
    "    dom.append(\"</table>\")\n",
    "    html = HTML(\"\".join(dom))\n",
    "    display(html)\n",
    "\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL_MAP[full_liar[k]['label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "html2 = HTML(\"\"\"<p>Please select the option that most closely describes the following claim by Jorge Elorza:<br>\n",
    "The reality is that we have roughly 15,000 undocumented immigrants living in the state...<br><br>\n",
    "\n",
    "A) True<br>\n",
    "B) Mostly True<br>\n",
    "C) Half True<br>\n",
    "D) Barely True<br>\n",
    "E) False<br>\n",
    "F) Pants on Fire (absurd lie)<br>\n",
    "<br>\n",
    "Choice: (A) True\n",
    "<br><br>\n",
    "Explanation: Jorge Elorza's statement is based on a report from the Rhode Island Department of Health, which estimated that there were approximately 15,000 undocumented immigrants living in the state in 2013. This estimate was derived from data on the number of uninsured residents, assuming a certain percentage were undocumented. While there may be some margin of error in this estimate, it is generally accepted as a reliable approximation of the number of undocumented immigrants in Rhode Island. Therefore, Jorge Elorza's statement is true.</s> \n",
    "<br><br>\n",
    "Explain your answer:\n",
    "<br><br>\n",
    "Jorge Elorza's statement that \"we have roughly 15,000 undocumented immigrants living in the state\" is based on a report from the Rhode Island Department of Health. The report estimated the number of undocumented immigrants in the state based on data on the number of uninsured residents, assuming a certain percentage were undocumented. This estimate is generally accepted as a reliable approximation of the number of undocumented immigrants in Rhode Island. Therefore, Jorge Elorza's statement is true.</p>\"\"\")\n",
    "display(html2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>Predicted Label</th><th>Attribution Label</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>A (0.53)</b></text></td><td><text style=\"padding-right:2em\"><b>A (0.53)</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #s                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Please                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> select                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> option                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> that                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> most                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> closely                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> describes                    </font></mark><mark style=\"background-color: hsl(0, 75%, 82%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 73%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> following                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> claim                    </font></mark><mark style=\"background-color: hsl(0, 75%, 72%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> by                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Jorge                    </font></mark><mark style=\"background-color: hsl(120, 75%, 77%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> El                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> or                    </font></mark><mark style=\"background-color: hsl(120, 75%, 80%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> za                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> :                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \n",
       "                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> The                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> reality                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> that                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> we                    </font></mark><mark style=\"background-color: hsl(0, 75%, 74%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> have                    </font></mark><mark style=\"background-color: hsl(0, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> roughly                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 1                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 5                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 0                    </font></mark><mark style=\"background-color: hsl(120, 75%, 65%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 0                    </font></mark><mark style=\"background-color: hsl(120, 75%, 65%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 0                    </font></mark><mark style=\"background-color: hsl(120, 75%, 72%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> und                    </font></mark><mark style=\"background-color: hsl(120, 75%, 69%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ocument                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ed                    </font></mark><mark style=\"background-color: hsl(120, 75%, 78%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> immigrants                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> living                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 81%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> state                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ...                    </font></mark><mark style=\"background-color: hsl(120, 75%, 62%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \n",
       "                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \n",
       "                    </font></mark><mark style=\"background-color: hsl(120, 75%, 68%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> A                    </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> )                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> True                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \n",
       "                    </font></mark><mark style=\"background-color: hsl(0, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> B                    </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> )                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Most                    </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ly                    </font></mark><mark style=\"background-color: hsl(120, 75%, 77%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> True                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \n",
       "                    </font></mark><mark style=\"background-color: hsl(120, 75%, 60%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> C                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> )                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Half                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> True                    </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \n",
       "                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> D                    </font></mark><mark style=\"background-color: hsl(0, 75%, 80%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> )                    </font></mark><mark style=\"background-color: hsl(0, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> B                    </font></mark><mark style=\"background-color: hsl(0, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> arely                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> True                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \n",
       "                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> E                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> )                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> False                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \n",
       "                    </font></mark><mark style=\"background-color: hsl(0, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> F                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> )                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> P                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ants                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> on                    </font></mark><mark style=\"background-color: hsl(0, 75%, 72%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Fire                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> (                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> abs                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> urd                    </font></mark><mark style=\"background-color: hsl(120, 75%, 62%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> lie                    </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> )                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \n",
       "                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \n",
       "                    </font></mark><mark style=\"background-color: hsl(0, 75%, 60%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Choice                    </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> :                    </font></mark><mark style=\"background-color: hsl(0, 75%, 60%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> (                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n"
     ]
    }
   ],
   "source": [
    "html = visualize_text(attr_vis)\n",
    "print(\"Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14392"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(f\"{model_name[model_name.index('/')+1:]}_{EXPERIMENT_TYPE.lower()}_side.html\", \"w\").write(html.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "# save pts as well\n",
    "torch.save(\n",
    "    OrderedDict(attributions),\n",
    "    f\"{model_name[model_name.index('/')+1:]}_{EXPERIMENT_TYPE.lower()}.pt\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
