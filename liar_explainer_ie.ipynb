{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrated Gradients Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/016854656/miniconda3/envs/torch/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context'],\n",
       "    num_rows: 12836\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "liar = datasets.load_dataset(\"liar\")\n",
    "full_liar = datasets.concatenate_datasets(\n",
    "    [liar[\"train\"], liar[\"test\"], liar[\"validation\"]]\n",
    ")\n",
    "full_liar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import captum\n",
    "import captum.attr as attr\n",
    "from captum.attr import visualization as viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon = \"tiiuae/falcon-7b-instruct\"\n",
    "llama = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "mistral = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "orca = \"microsoft/Orca-2-7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f68ca7f08a49109a465f00c7cc961e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, quantization_config=config, # device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "LABEL_MAP = {\n",
    "    0: \"E\",  # 0 : False\n",
    "    1: \"C\",  # 1 : Half True\n",
    "    2: \"B\",  # 2 : Mostly True\n",
    "    3: \"A\",  # 3 : True\n",
    "    4: \"D\",  # 4 : Barely True\n",
    "    5: \"F\",  # 5 : Pants on Fire\n",
    "}\n",
    "\n",
    "\n",
    "def was_correct(decoded: str, entry: Dict[str, int]) -> bool:\n",
    "    return LABEL_MAP[entry[\"label\"]] in decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Dict\n",
    "\n",
    "entries = random.choices(list(range(len(full_liar))), k=n_examples)\n",
    "\n",
    "\n",
    "def to_zero_shot_prompt(entry: Dict[str, str]) -> str:\n",
    "    speaker = entry[\"speaker\"].replace(\"-\", \" \").title()\n",
    "    try:\n",
    "        statement = entry['statement'][entry[\"statement\"].index(\"Says \")+5:]\n",
    "    except:\n",
    "        statement = entry['statement']\n",
    "\n",
    "    prompt = f\"\"\"Please select the option that most closely describes the following claim by {speaker}:\\n{statement}\\n\\nA) True\\nB) Mostly True\\nC) Half True\\nD) Barely True\\nE) False\\nF) Pants on Fire (absurd lie)\\n\\nChoice: (\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def to_n_shot_prompt(n: int, entry: Dict[str, str]) -> str:\n",
    "    examples = \"\"\n",
    "    for i in range(n):\n",
    "        examples += (\n",
    "            to_zero_shot_prompt(full_liar[entries[i]])\n",
    "            + LABEL_MAP[full_liar[entries[i]][\"label\"]]\n",
    "            + \"\\n\\n\"\n",
    "        )\n",
    "    prompt = to_zero_shot_prompt(entry)\n",
    "    return examples + prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': [28741],\n",
       " 'B': [28760],\n",
       " 'C': [28743],\n",
       " 'D': [28757],\n",
       " 'E': [28749],\n",
       " 'F': [28765]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = tokenizer.vocab\n",
    "label_tokens = {\n",
    "    \"A\": [],\n",
    "    \"B\": [],\n",
    "    \"C\": [],\n",
    "    \"D\": [],\n",
    "    \"E\": [],\n",
    "    \"F\": [],\n",
    "}\n",
    "for char, idx in vocab.items():\n",
    "    if char in label_tokens:\n",
    "        label_tokens[char].append(idx)\n",
    "label_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 28741, 'B': 28760, 'C': 28743, 'D': 28757, 'E': 28749, 'F': 28765}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_tokens = {char: idxs[0] for char, idxs in label_tokens.items()}\n",
    "label_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = to_n_shot_prompt(n_examples, full_liar[random.randint(0, len(full_liar))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please select the option that most closely describes the following claim by Marco Rubio:\n",
      "If people work and make more money, they lose more in benefits than they would earn in salary.\n",
      "\n",
      "A) True\n",
      "B) Mostly True\n",
      "C) Half True\n",
      "D) Barely True\n",
      "E) False\n",
      "F) Pants on Fire (absurd lie)\n",
      "\n",
      "Choice: (\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 81])\n"
     ]
    }
   ],
   "source": [
    "# get tokens for later\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "print(tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28760, device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    label = torch.argmax(model(tokens.cuda()).logits[:, -1])\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = tokenizer.decode(label)\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_results(inputs: torch.Tensor):\n",
    "    result = model(inputs.cuda()).logits\n",
    "    return torch.nn.functional.softmax(result[:, -1], dim=-1).cpu()\n",
    "def softmax_results_embeds(embds: torch.Tensor):\n",
    "    result = model(inputs_embeds=embds.cuda()).logits\n",
    "    return torch.nn.functional.softmax(result[:, -1], dim=-1).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/016854656/miniconda3/envs/torch/lib/python3.11/site-packages/captum/attr/_models/base.py:191: UserWarning: In order to make embedding layers more interpretable they will be replaced with an interpretable embedding layer which wraps the original embedding layer and takes word embedding vectors as inputs of the forward function. This allows us to generate baselines for word embeddings and compute attributions for each embedding dimension. The original embedding layer must be set back by calling `remove_interpretable_embedding_layer` function after model interpretation is finished. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): InterpretableEmbeddingBase(\n",
       "      (embedding): Embedding(32000, 4096)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace the normal pytorch embeddings (which only take ints) to interpretable embeddings\n",
    "# (which are compatible with the float inputs that integratedgradients gives)\n",
    "if hasattr(model, \"model\"):\n",
    "    interpretable_emb = attr.configure_interpretable_embedding_layer(\n",
    "        model.model, \"embed_tokens\"\n",
    "    )\n",
    "elif hasattr(model, \"transformer\"):\n",
    "    interpretable_emb = attr.configure_interpretable_embedding_layer(\n",
    "        model.transformer, \"word_embeddings\"\n",
    "    )\n",
    "else:\n",
    "    print(\"What happened\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 81, 4096])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embs = interpretable_emb.indices_to_embeddings(tokens).cpu()\n",
    "print(input_embs.device)\n",
    "input_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 81, 4096])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baselines = torch.zeros_like(input_embs).cpu()\n",
    "print(baselines.device)\n",
    "baselines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = attr.IntegratedGradients(softmax_results_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions = ig.attribute(\n",
    "    input_embs,\n",
    "    baselines=baselines,\n",
    "    target=label_tokens[label],\n",
    "    n_steps=512,\n",
    "    internal_batch_size=2,\n",
    "    return_convergence_delta=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 81, 4096])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_attributions(attributions):\n",
    "    with torch.no_grad():\n",
    "        attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "        print((attributions / torch.norm(attributions)).sum())\n",
    "        return attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8090, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "summarized_attributions = summarize_attributions(attributions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([81])\n",
      "['<s>', 'Please', 'select', 'the', 'option', 'that', 'most', 'closely', 'describes', 'the', 'following', 'claim', 'by', 'Marco', 'Rub', 'io', ':', '\\n', 'If', 'people', 'work', 'and', 'make', 'more', 'money', ',', 'they', 'lose', 'more', 'in', 'benefits', 'than', 'they', 'would', 'earn', 'in', 'salary', '.', '\\n', '\\n', 'A', ')', 'True', '\\n', 'B', ')', 'Most', 'ly', 'True', '\\n', 'C', ')', 'Half', 'True', '\\n', 'D', ')', 'B', 'arely', 'True', '\\n', 'E', ')', 'False', '\\n', 'F', ')', 'P', 'ants', 'on', 'Fire', '(', 'abs', 'urd', 'lie', ')', '\\n', '\\n', 'Choice', ':', '(']\n"
     ]
    }
   ],
   "source": [
    "all_tokens = tokens.squeeze(0)\n",
    "print(all_tokens.shape)\n",
    "all_tokens = list(map(tokenizer.decode, all_tokens))\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32000])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the interpretable embedding layer so we can get regular predictions\n",
    "if hasattr(model, \"model\"):\n",
    "    attr.remove_interpretable_embedding_layer(model.model, interpretable_emb)\n",
    "else:\n",
    "    attr.remove_interpretable_embedding_layer(model.transformer, interpretable_emb)\n",
    "with torch.no_grad():\n",
    "    predictions = softmax_results(tokens)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for B\n",
      "we are pretty close!\n",
      "got tensor(0.6054, dtype=torch.float64) instead of tensor(0.5734)\n"
     ]
    }
   ],
   "source": [
    "MARGIN_OF_ERROR = 0.1  # off by no more than 10 percent\n",
    "\n",
    "print(\"for\", label)\n",
    "if (\n",
    "    torch.abs(\n",
    "        (summarized_attributions.sum() - predictions[0, label_tokens[label]])\n",
    "    )\n",
    "    >= MARGIN_OF_ERROR\n",
    "):\n",
    "    print(\"we are off!!\")\n",
    "    print(\"we should be getting somewhere near\", predictions[0, label_tokens[label]])\n",
    "    print(\"instead, we get\", summarized_attributions[label].sum())\n",
    "else:\n",
    "    print(\"we are pretty close!\")\n",
    "    print(\n",
    "        \"got\",\n",
    "        summarized_attributions.sum(),\n",
    "        \"instead of\",\n",
    "        predictions[0, label_tokens[label]],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CustomDataRecord:\n",
    "    word_attributions: Any\n",
    "    pred_prob: torch.Tensor\n",
    "    pred_class: str\n",
    "    attr_class: str\n",
    "    attr_prob: torch.Tensor\n",
    "    attr_score: Any\n",
    "    raw_input_ids: Any | list[str]\n",
    "    convergence_delta: Any\n",
    "\n",
    "# SCALE = {char:10 for char in summarized_attributions}\n",
    "SCALE=2/summarized_attributions.max()\n",
    "\n",
    "attr_vis = CustomDataRecord(\n",
    "        summarized_attributions *SCALE,  # word attributions\n",
    "        predictions[0].max(),  # predicted probability\n",
    "        tokenizer.decode(torch.argmax(predictions[0])),  # predicted class\n",
    "        label,  # attr class\n",
    "        predictions[0, label_tokens[label]],  # attr probability\n",
    "        summarized_attributions.sum(),  # attr score\n",
    "        all_tokens,  # raw input ids\n",
    "        attributions[1],  # convergence delta\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "def _get_color(attr):\n",
    "    # clip values to prevent CSS errors (Values should be from [-1,1])\n",
    "    attr = max(-2, min(2, attr))\n",
    "    if attr > 0:\n",
    "        hue = 120\n",
    "        sat = 75\n",
    "        lig = 100 - int(50 * attr)\n",
    "    else:\n",
    "        hue = 0\n",
    "        sat = 75\n",
    "        lig = 100 - int(-40 * attr)\n",
    "    return \"hsl({}, {}%, {}%)\".format(hue, sat, lig)\n",
    "\n",
    "def visualize_text(\n",
    "    datarecords: list[CustomDataRecord], legend: bool = True\n",
    ") -> \"HTML\":  # In quotes because this type doesn't exist in standalone mode\n",
    "    dom = [\"<table width: 100%>\"]\n",
    "    rows = [\n",
    "        \"<tr><th>Predicted Label</th>\"\n",
    "        \"<th>Attribution Label</th>\"\n",
    "        \"<th>Convergence Delta</th>\"\n",
    "        \"<th>Attribution Score</th>\"\n",
    "        \"<th>Word Importance</th>\"\n",
    "    ]\n",
    "    for datarecord in datarecords:\n",
    "        rows.append(\n",
    "            \"\".join(\n",
    "                [\n",
    "                    \"<tr>\",\n",
    "                    viz.format_classname(\n",
    "                        \"{0} ({1:.2f})\".format(\n",
    "                            datarecord.pred_class, datarecord.pred_prob\n",
    "                        )\n",
    "                    ),\n",
    "                    viz.format_classname(\n",
    "                        \"{0} ({1:.2f})\".format(\n",
    "                            datarecord.attr_class, datarecord.attr_prob\n",
    "                        )\n",
    "                    ),\n",
    "                    viz.format_classname(\n",
    "                        \"{0:.2f}\".format(datarecord.convergence_delta.item())\n",
    "                    ),\n",
    "                    viz.format_classname(\"{0:.2f}\".format(datarecord.attr_score)),\n",
    "                    viz.format_word_importances(\n",
    "                        datarecord.raw_input_ids, datarecord.word_attributions\n",
    "                    ),\n",
    "                    \"<tr>\",\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if legend:\n",
    "        dom.append(\n",
    "            '<div style=\"border-top: 1px solid; margin-top: 5px; \\\n",
    "            padding-top: 5px; display: inline-block\">'\n",
    "        )\n",
    "        dom.append(\"<b>Legend: </b>\")\n",
    "\n",
    "        for value, label in zip([-1, 0, 1], [\"Negative\", \"Neutral\", \"Positive\"]):\n",
    "            dom.append(\n",
    "                '<span style=\"display: inline-block; width: 10px; height: 10px; \\\n",
    "                border: 1px solid; background-color: \\\n",
    "                {value}\"></span> {label}  '.format(\n",
    "                    value=_get_color(value), label=label\n",
    "                )\n",
    "            )\n",
    "        dom.append(\"</div>\")\n",
    "\n",
    "    dom.append(\"\".join(rows))\n",
    "    dom.append(\"</table>\")\n",
    "    html = HTML(\"\".join(dom))\n",
    "    display(html)\n",
    "\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>Predicted Label</th><th>Attribution Label</th><th>Convergence Delta</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>B (0.57)</b></text></td><td><text style=\"padding-right:2em\"><b>B (0.57)</b></text></td><td><text style=\"padding-right:2em\"><b>0.03</b></text></td><td><text style=\"padding-right:2em\"><b>0.61</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #s                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Please                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> select                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> option                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> that                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> most                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> closely                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> describes                    </font></mark><mark style=\"background-color: hsl(0, 75%, 63%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 77%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> following                    </font></mark><mark style=\"background-color: hsl(120, 75%, 66%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> claim                    </font></mark><mark style=\"background-color: hsl(120, 75%, 56%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> by                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Marco                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Rub                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> io                    </font></mark><mark style=\"background-color: hsl(0, 75%, 76%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> :                    </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \n",
       "                    </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> If                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> people                    </font></mark><mark style=\"background-color: hsl(0, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> work                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 82%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> make                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> more                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> money                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> they                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> lose                    </font></mark><mark style=\"background-color: hsl(0, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> more                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> benefits                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> than                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> they                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> would                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> earn                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> salary                    </font></mark><mark style=\"background-color: hsl(120, 75%, 70%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 65%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \n",
       "                    </font></mark><mark style=\"background-color: hsl(120, 75%, 67%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \n",
       "                    </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> A                    </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> )                    </font></mark><mark style=\"background-color: hsl(0, 75%, 63%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> True                    </font></mark><mark style=\"background-color: hsl(0, 75%, 60%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \n",
       "                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> B                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> )                    </font></mark><mark style=\"background-color: hsl(120, 75%, 71%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Most                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ly                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> True                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \n",
       "                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> C                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> )                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Half                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> True                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \n",
       "                    </font></mark><mark style=\"background-color: hsl(0, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> D                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> )                    </font></mark><mark style=\"background-color: hsl(120, 75%, 65%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> B                    </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> arely                    </font></mark><mark style=\"background-color: hsl(0, 75%, 73%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> True                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \n",
       "                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> E                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> )                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> False                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \n",
       "                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> F                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> )                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> P                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ants                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> on                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Fire                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> (                    </font></mark><mark style=\"background-color: hsl(0, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> abs                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> urd                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> lie                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> )                    </font></mark><mark style=\"background-color: hsl(0, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \n",
       "                    </font></mark><mark style=\"background-color: hsl(0, 75%, 73%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \n",
       "                    </font></mark><mark style=\"background-color: hsl(0, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Choice                    </font></mark><mark style=\"background-color: hsl(120, 75%, 73%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> :                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> (                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n"
     ]
    }
   ],
   "source": [
    "html = visualize_text([attr_vis])\n",
    "print(\"Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tokenizer.batch_decode(model.generate(tokens.cuda(), max_length=100, pad_token_id=tokenizer.eos_token_id))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explaining_prompt = \"Please explain why the claim by Dennis Kucinich that \\\"We are giving almost $2 billion of taxpayer money to the junk food and fast food industries every year to make the (childhood obesity) epidemic worse.\\\" is mostly true.\\n\"\n",
    "# print(tokenizer.batch_decode(model.generate(tokenizer(explaining_prompt, return_tensors='pt').input_ids.cuda(), max_length=200, pad_token_id=tokenizer.eos_token_id))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13927"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(f\"{model_name[model_name.index('/')+1:]}_0.html\", \"w\").write(html.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
