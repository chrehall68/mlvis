{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting on Liar Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]=\"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context'],\n",
       "        num_rows: 10269\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context'],\n",
       "        num_rows: 1283\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context'],\n",
       "        num_rows: 1284\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "liar = datasets.load_dataset(\"liar\")\n",
    "liar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = liar[\"train\"]\n",
    "test = liar[\"test\"]\n",
    "val = liar[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/016854656/miniconda3/envs/torch/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context'],\n",
       "    num_rows: 12836\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_liar = datasets.concatenate_datasets([train, test, val])\n",
    "full_liar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon = \"tiiuae/falcon-7b-instruct\"\n",
    "llama = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "mistral = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "orca = \"microsoft/Orca-2-7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this depending on experiment\n",
    "model_name = mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "import accelerate\n",
    "\n",
    "# config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name\n",
    ")\n",
    "with accelerate.init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_config(config, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralModel(\n",
      "  (embed_tokens): Embedding(32000, 4096)\n",
      "  (layers): ModuleList(\n",
      "    (0-31): 32 x MistralDecoderLayer(\n",
      "      (self_attn): MistralSdpaAttention(\n",
      "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (rotary_emb): MistralRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): MistralMLP(\n",
      "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): MistralRMSNorm()\n",
      "      (post_attention_layernorm): MistralRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): MistralRMSNorm()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if model_name == falcon:\n",
    "    print(model.transformer)\n",
    "else:\n",
    "    print(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('model.embed_tokens', 0),\n",
       "             ('model.layers.0', 0),\n",
       "             ('model.layers.1', 0),\n",
       "             ('model.layers.2', 0),\n",
       "             ('model.layers.3', 0),\n",
       "             ('model.layers.4', 0),\n",
       "             ('model.layers.5', 0),\n",
       "             ('model.layers.6', 0),\n",
       "             ('model.layers.7', 0),\n",
       "             ('model.layers.8', 0),\n",
       "             ('model.layers.9', 0),\n",
       "             ('model.layers.10', 0),\n",
       "             ('model.layers.11', 0),\n",
       "             ('model.layers.12', 0),\n",
       "             ('model.layers.13', 0),\n",
       "             ('model.layers.14', 0),\n",
       "             ('model.layers.15', 0),\n",
       "             ('model.layers.16', 0),\n",
       "             ('model.layers.17', 0),\n",
       "             ('model.layers.18', 0),\n",
       "             ('model.layers.19', 0),\n",
       "             ('model.layers.20', 0),\n",
       "             ('model.layers.21', 0),\n",
       "             ('model.layers.22', 1),\n",
       "             ('model.layers.23', 1),\n",
       "             ('model.layers.24', 1),\n",
       "             ('model.layers.25', 1),\n",
       "             ('model.layers.26', 1),\n",
       "             ('model.layers.27', 1),\n",
       "             ('model.layers.28', 1),\n",
       "             ('model.layers.29', 1),\n",
       "             ('model.layers.30', 1),\n",
       "             ('model.layers.31', 1),\n",
       "             ('model.norm', 1),\n",
       "             ('lm_head', 1)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import accelerate\n",
    "model.tie_weights()\n",
    "dev_map = accelerate.infer_auto_device_map(model, max_memory={0:\"11GB\", 1:\"7GB\"}, no_split_module_classes=[\"LlamaDecoderLayer\", \"MistralDecoderLayer\", \"FalconDecoderLayer\"])\n",
    "dev_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b21d56fa1f94548b5e283516f3ed968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=dev_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "LABEL_MAP = {\n",
    "    0: \"E\",  # 0 : False\n",
    "    1: \"C\",  # 1 : Half True\n",
    "    2: \"B\",  # 2 : Mostly True\n",
    "    3: \"A\",  # 3 : True\n",
    "    4: \"D\",  # 4 : Barely True\n",
    "    5: \"F\",  # 5 : Pants on Fire\n",
    "}\n",
    "\n",
    "\n",
    "def was_correct(\n",
    "    decoded:str, entry: Dict[str, int]\n",
    ") -> bool:\n",
    "    return LABEL_MAP[entry[\"label\"]] in decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = []\n",
    "for idx, item in enumerate(full_liar):\n",
    "    if item['context'] == 'none' or item['context'] == '':\n",
    "        idxs.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '610.json',\n",
       " 'label': 0,\n",
       " 'statement': 'On using the Strategic Petroleum Reserve.',\n",
       " 'subject': 'energy',\n",
       " 'speaker': 'barack-obama',\n",
       " 'job_title': 'President',\n",
       " 'state_info': 'Illinois',\n",
       " 'party_affiliation': 'democrat',\n",
       " 'barely_true_counts': 70.0,\n",
       " 'false_counts': 71.0,\n",
       " 'half_true_counts': 160.0,\n",
       " 'mostly_true_counts': 163.0,\n",
       " 'pants_on_fire_counts': 9.0,\n",
       " 'context': ''}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_liar[idxs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List\n",
    "random.seed(1770)\n",
    "entries = random.choices(list(range(len(train))), k=n_examples)\n",
    "\n",
    "def to_zero_shot_prompt(entry: Dict[str, str]) -> str:\n",
    "    speaker = entry[\"speaker\"].replace(\"-\", \" \").title()\n",
    "    statement = entry[\"statement\"].lstrip(\"Says \")\n",
    "    context = entry['context']\n",
    "    if context != '':\n",
    "        context = f\" made in {context}\"\n",
    "    party = entry['party_affiliation']\n",
    "    if party != '':\n",
    "        party = f\"{party} \"\n",
    "\n",
    "    prompt = f\"\"\"Please select the option that most closely describes the following claim by {party}{speaker}{context}:\\n{statement}\\n\\nA) True\\nB) Mostly True\\nC) Half True\\nD) Barely True\\nE) False\\nF) Pants on Fire (absurd lie)\\n\\nChoice: (\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def to_n_shot_prompt(n: int, entry: Dict[str, str]) -> str:\n",
    "    examples = \"\"\n",
    "    for i in range(n):\n",
    "        examples += to_zero_shot_prompt(train[entries[i]]) + LABEL_MAP[train[entries[i]]['label']] + \"\\n\\n\"\n",
    "    prompt = to_zero_shot_prompt(entry)\n",
    "    return examples + prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses: Dict[str, list] = {}\n",
    "\n",
    "def workflow(idx:int, entry: dict, model, k:int=0, verbose: bool = False) -> bool:\n",
    "    # encode input, move it to cuda, then generate\n",
    "    encoded_input = tokenizer(entry['prompt'], return_tensors=\"pt\")\n",
    "    encoded_input = {item: val.cuda() for item, val in encoded_input.items()}\n",
    "    generation = model.generate(\n",
    "        **encoded_input,\n",
    "        max_new_tokens=1,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # log the prompt and response if verbose\n",
    "    if verbose:\n",
    "        print(tokenizer.batch_decode(generation)[0])\n",
    "\n",
    "    decoded = tokenizer.decode(generation[0, -1])\n",
    "    correct = was_correct(decoded, entry)\n",
    "    \n",
    "    if decoded not in responses:\n",
    "        responses[decoded] = []\n",
    "    responses[decoded].append(idx)\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"The model was\",\n",
    "            \"correct\" if correct else \"incorrect\",\n",
    "            \" - responded\",\n",
    "            tokenizer.decode(generation[0, -1]),\n",
    "            \"and answer should have been\",\n",
    "            LABEL_MAP[entry[\"label\"]],\n",
    "        )\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context', 'prompt'],\n",
       "    num_rows: 12836\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_liar = full_liar.map(lambda e: {\"prompt\":to_n_shot_prompt(n_examples, e)})\n",
    "full_liar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Please select the option that most closely describes the following claim by independent Bernie S made in a message on Twitter:\n",
      "Today the Walton family of Walmart own more wealth than the bottom 40 percent of America.\n",
      "\n",
      "A) True\n",
      "B) Mostly True\n",
      "C) Half True\n",
      "D) Barely True\n",
      "E) False\n",
      "F) Pants on Fire (absurd lie)\n",
      "\n",
      "Choice: (A\n",
      "\n",
      "Please select the option that most closely describes the following claim by republican Kim Guadagno made in a radio interview on WNYC's Brian Lehrer Show:\n",
      "Panasonic stayed in New Jersey because of the Business Retention and Relocation Assistance Grant (BRRAG) program.\n",
      "\n",
      "A) True\n",
      "B) Mostly True\n",
      "C) Half True\n",
      "D) Barely True\n",
      "E) False\n",
      "F) Pants on Fire (absurd lie)\n",
      "\n",
      "Choice: (E\n",
      "\n",
      "Please select the option that most closely describes the following claim by democrat Democratic Governors Association made in a statement on a DGA-created website:\n",
      "Lincoln Chafee voted with President George W. Bush and the conservative leadership 76% of the time.\n",
      "\n",
      "A) True\n",
      "B) Mostly True\n",
      "C) Half True\n",
      "D) Barely True\n",
      "E) False\n",
      "F) Pants on Fire (absurd lie)\n",
      "\n",
      "Choice: (D\n",
      "\n",
      "Please select the option that most closely describes the following claim by democrat Patrick Kennedy made in a speech on the House floor. :\n",
      "\"One-third of the health care dollar goes to no such thing as health care; it goes to the insurance companies.\"\n",
      "\n",
      "A) True\n",
      "B) Mostly True\n",
      "C) Half True\n",
      "D) Barely True\n",
      "E) False\n",
      "F) Pants on Fire (absurd lie)\n",
      "\n",
      "Choice: (E\n",
      "\n",
      "Please select the option that most closely describes the following claim by democrat Hillary Clinton made in a speech in Cincinnati:\n",
      "Risk analysts listed Donald Trump, a Donald Trump presidency, as one of the top threats facing the global economy, ahead of terrorism.\n",
      "\n",
      "A) True\n",
      "B) Mostly True\n",
      "C) Half True\n",
      "D) Barely True\n",
      "E) False\n",
      "F) Pants on Fire (absurd lie)\n",
      "\n",
      "Choice: (A\n",
      "\n",
      "Please select the option that most closely describes the following claim by democrat Roy Barnes made in a campaign commercial:\n",
      "When Roy Barnes was governor, \"Georgia created 235,000 jobs.\"\n",
      "\n",
      "A) True\n",
      "B) Mostly True\n",
      "C) Half True\n",
      "D) Barely True\n",
      "E) False\n",
      "F) Pants on Fire (absurd lie)\n",
      "\n",
      "Choice: (B\n",
      "The model was incorrect  - responded B and answer should have been C\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "workflow(0, full_liar[random.randint(0, len(full_liar) - 1)], model, verbose=True, k=n_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of zero-shot prompting the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–Ž                          | 176/12836 [00:37<45:29,  4.64it/s, acc: 0.233]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m entries:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't include items that were in the examples\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[43mworkflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_examples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m correct:\n\u001b[1;32m     11\u001b[0m     num_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m, in \u001b[0;36mworkflow\u001b[0;34m(idx, entry, model, k, verbose)\u001b[0m\n\u001b[1;32m      5\u001b[0m encoded_input \u001b[38;5;241m=\u001b[39m tokenizer(entry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m encoded_input \u001b[38;5;241m=\u001b[39m {item: val\u001b[38;5;241m.\u001b[39mcuda() \u001b[38;5;28;01mfor\u001b[39;00m item, val \u001b[38;5;129;01min\u001b[39;00m encoded_input\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m----> 7\u001b[0m generation \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoded_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# log the prompt and response if verbose\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/transformers/generation/utils.py:1544\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1527\u001b[0m         input_ids,\n\u001b[1;32m   1528\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1541\u001b[0m     )\n\u001b[1;32m   1542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1543\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/transformers/generation/utils.py:2404\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2401\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2403\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2404\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2405\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2407\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2408\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2409\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2412\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:1157\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1154\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1157\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1169\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1170\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:1042\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1033\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1034\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1039\u001b[0m         use_cache,\n\u001b[1;32m   1040\u001b[0m     )\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1042\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1051\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:757\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    754\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    756\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    765\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    767\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:664\u001b[0m, in \u001b[0;36mMistralSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    663\u001b[0m     kv_seq_len \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m past_key_value\u001b[38;5;241m.\u001b[39mget_usable_length(kv_seq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_idx)\n\u001b[0;32m--> 664\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotary_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_seq_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    666\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n\u001b[1;32m    668\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:124\u001b[0m, in \u001b[0;36mMistralRotaryEmbedding.forward\u001b[0;34m(self, x, seq_len)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seq_len \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_len_cached:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_cos_sin_cache(seq_len\u001b[38;5;241m=\u001b[39mseq_len, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 124\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcos_cached\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msin_cached[:seq_len]\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdtype),\n\u001b[1;32m    126\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_correct = 0\n",
    "responses={}\n",
    "for idx, entry in enumerate((prog:=tqdm(full_liar))):\n",
    "    if idx in entries:\n",
    "        continue  # don't include items that were in the examples\n",
    "    \n",
    "    correct = workflow(idx, entry, model, k=n_examples)\n",
    "    if correct:\n",
    "        num_correct += 1\n",
    "    prog.set_postfix_str(f\"acc: {num_correct/(idx+1):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['D', 'B', 'E', 'A', 'C', 'M', 'T', 'I', 'F', 'Assembly'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "\n",
    "df = {\"labels\":[]}\n",
    "for key, val in responses.items():\n",
    "    df['labels'].extend([key]*len(val))\n",
    "df = pd.DataFrame.from_dict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='labels', ylabel='count'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(df, x='labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'meta-llama/Llama-2-7b-chat-hf 0-shot')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accepted = ['A', 'B', 'C', 'D', 'E','F']\n",
    "filtered = df.map(lambda val: val if val in accepted else 'UNK')\n",
    "sns.countplot(filtered, x='labels', ).set_title(f\"{model_name} 0-shot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(responses, open(f\"{model_name[model_name.index('/')+1:]}_{n_examples}responses_full.pk\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log results\n",
    "with open(f\"{n_examples}_shot_full.txt\", \"a\") as file:\n",
    "    file.write(f\"{model_name} : {num_correct}/{len(full_liar)-len(entries)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-2-7b-chat-hf : 2384/12836\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print results up till now\n",
    "with open(f\"{n_examples}_shot_full.txt\", \"r\") as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
